{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pair Trading with Kalman Filter - Dynamic Hedge Ratio Analysis\n",
        "\n",
        "## Objective\n",
        "This exercise aims to use a Kalman filter to estimate the **dynamic hedge ratio** between pairs of related stocks for potential **pair trading**. Your goal is to identify which pair of stocks has the highest probability of yielding profitable trading signals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from datetime import date\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1 - Data Preprocessing\n",
        "1. Load and verify the quality of the data.\n",
        "2. Aggregate the data to x-minute interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data(): \n",
        "    A = pd.read_csv(r\"data\\A.csv\")\n",
        "    B = pd.read_csv(r\"data\\B.csv\")\n",
        "    C = pd.read_csv(r\"data\\C.csv\")\n",
        "    return A, B,C \n",
        "\n",
        "def data_analysis(df, name = \"X\"):\n",
        "    # understanding data \n",
        "    print(\"Data Description:\", \"\\n\")\n",
        "    print(df.describe())\n",
        "\n",
        "    require = [\"exg_time\",\"trade_price\",\"trade_qty\"]\n",
        "    missing = [c for c in require if c not in df.columns]\n",
        "    if missing:\n",
        "        return {\"name\": name, \"error\": f\"missing columns: {missing}\"}\n",
        "\n",
        "    d = df.copy()\n",
        "    # time stamp analysis \n",
        "    d[\"exg_time\"] = pd.to_datetime(d[\"exg_time\"], utc=True, errors=\"coerce\")\n",
        "    d = d.sort_values(\"exg_time\")\n",
        "\n",
        "    out = {}\n",
        "    out[\"name\"] = name\n",
        "    out[\"rows\"] = len(d)\n",
        "\n",
        "    # nan values, missing values \n",
        "    out[\"null_exg_time\"] = int(d[\"exg_time\"].isna().sum())\n",
        "    out[\"null_price\"]    = int(d[\"trade_price\"].isna().sum())\n",
        "    out[\"null_qty\"]      = int(d[\"trade_qty\"].isna().sum())\n",
        "    out[\"nonpos_price\"]  = int((d[\"trade_price\"] <= 0).sum())\n",
        "    out[\"nonpos_qty\"]    = int((d[\"trade_qty\"] <= 0).sum())\n",
        "\n",
        "    # time range\n",
        "    out[\"start\"] = str(d[\"exg_time\"].min())\n",
        "    out[\"end\"]   = str(d[\"exg_time\"].max())\n",
        "\n",
        "    # check exact duplicated rows \n",
        "    keys = [\"exg_time\", \"trade_price\", \"trade_qty\"]\n",
        "\n",
        "    dup_mask = df.duplicated(subset=keys, keep=\"first\")   # marks ONLY the later repeats\n",
        "    dup_count = int(dup_mask.sum())\n",
        "    out[\"duplicated rows\"] = dup_count \n",
        "\n",
        "    return out\n",
        "\n",
        "def data_cleaning(df):\n",
        "    TICK_KEYS = [\"exg_time\", \"trade_price\", \"trade_qty\"]\n",
        "    d = df.copy()\n",
        "    d[\"exg_time\"] = pd.to_datetime(d[\"exg_time\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "    # drop null time\n",
        "    d = d.dropna(subset=[\"exg_time\"])\n",
        "\n",
        "    # drop negative price or negative qty\n",
        "    d = d[(d[\"trade_price\"] > 0) & (d[\"trade_qty\"] > 0)]\n",
        "\n",
        "    # drop exact duplicates (keep first only)\n",
        "    d = d.drop_duplicates(subset=TICK_KEYS, keep=\"first\")\n",
        "\n",
        "    # sort\n",
        "    d = d.sort_values(\"exg_time\").reset_index(drop=True)\n",
        "\n",
        "    return d \n",
        "\n",
        "\n",
        "def resample_data(df, x_min = 1):\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Description: \n",
            "\n",
            "         trade_price      trade_qty\n",
            "count  460248.000000  460248.000000\n",
            "mean    33513.581460     402.947759\n",
            "std      1378.703901     780.900078\n",
            "min     31600.000000     100.000000\n",
            "25%     32320.000000     100.000000\n",
            "50%     33430.000000     200.000000\n",
            "75%     34120.000000     400.000000\n",
            "max     36600.000000   72500.000000\n",
            "{'name': 'A', 'rows': 460248, 'null_exg_time': 6, 'null_price': 0, 'null_qty': 0, 'nonpos_price': 0, 'nonpos_qty': 0, 'start': '2024-01-02 01:00:00.150894+00:00', 'end': '2024-04-03 09:04:55.015295+00:00', 'duplicated rows': 134292}\n",
            "Data Description: \n",
            "\n",
            "         trade_price      trade_qty\n",
            "count  339588.000000  339588.000000\n",
            "mean    28559.525366     358.073901\n",
            "std       494.355500     706.291635\n",
            "min     27620.000000     100.000000\n",
            "25%     28200.000000     100.000000\n",
            "50%     28370.000000     200.000000\n",
            "75%     28910.000000     300.000000\n",
            "max     29880.000000  119900.000000\n",
            "{'name': 'B', 'rows': 339588, 'null_exg_time': 0, 'null_price': 0, 'null_qty': 0, 'nonpos_price': 0, 'nonpos_qty': 0, 'start': '2024-01-02 01:00:02.236499+00:00', 'end': '2024-04-03 09:04:55.015295+00:00', 'duplicated rows': 97534}\n",
            "Data Description: \n",
            "\n",
            "         trade_price      trade_qty\n",
            "count  316012.000000  316012.000000\n",
            "mean    13125.826646     724.803805\n",
            "std       284.795074    1674.265185\n",
            "min     12660.000000     100.000000\n",
            "25%     12900.000000     100.000000\n",
            "50%     13030.000000     300.000000\n",
            "75%     13340.000000     700.000000\n",
            "max     13840.000000  126000.000000\n",
            "{'name': 'C', 'rows': 316012, 'null_exg_time': 0, 'null_price': 0, 'null_qty': 0, 'nonpos_price': 0, 'nonpos_qty': 0, 'start': '2024-01-02 01:00:00.355780+00:00', 'end': '2024-04-03 09:04:55.015295+00:00', 'duplicated rows': 73806}\n"
          ]
        }
      ],
      "source": [
        "A, B, C = get_data()\n",
        "\n",
        "print(data_analysis(A, name = \"A\"))\n",
        "print(data_analysis(B, name = \"B\"))\n",
        "print(data_analysis(C, name = \"C\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments on Data \n",
        "* Data is somewhat clean, some null \"exg_time\" in A.\n",
        "* No null values in price and qty for all A,B,C\n",
        "* A LOT of duplicated rows (exact values for all 3 columns:[\"exg_time\", \"trade_price\", \"trade_qty\"]) \n",
        "* handled by dropping rows with null \"exg_time\" value + dropping all duplicated rows, only keeping the first instance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2 - Implement Kalman Filter for Hedge Ratio Estimation\n",
        "1. Write a function to apply the **Kalman filter** and estimate the hedge ratio between two stocks dynamically over time.\n",
        "2. Calculate the hedge ratio for each of the pairs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3 - Calculate the Spread for Each Pair\n",
        "1. Using the hedge ratio estimated in Task 2, compute a **spread** for each pair\n",
        "2. **Check Mean Reversion**: Before applying the strategy, check if the spread between the pairs shows signs of mean reversion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 4 - Simulate Trading Strategy and Profitability\n",
        "1. Define a simple **mean-reversion trading strategy**, assuming no trading cost.\n",
        "2. Calculate the **cumulative profit and loss (P&L)** for each pair based on this strategy.\n",
        "3. Record the cumulative P&L for each pair at the end of the period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "## trading class \n",
        "class TradingEngine:\n",
        "    def __init__(self, initial_capital=100000.0, commission_bps=0.0001):\n",
        "        self.cash = initial_capital\n",
        "        self.initial_capital = initial_capital\n",
        "        self.commission_bps = commission_bps\n",
        "        self.positions = {} # {ticker: {'qty': 0, 'avg_price': 0.0}}\n",
        "        self.history = []\n",
        "        self.equity_curve = [] # Track daily portfolio value\n",
        "\n",
        "    def get_portfolio_value(self, current_prices):\n",
        "        \"\"\"Returns Cash + Market Value of Positions\"\"\"\n",
        "        market_value = 0.0\n",
        "        for ticker, pos in self.positions.items():\n",
        "            if ticker in current_prices:\n",
        "                market_value += pos['qty'] * current_prices[ticker]\n",
        "        return self.cash + market_value\n",
        "\n",
        "    def execute_order(self, ticker, qty, price, date, action_label):\n",
        "        \"\"\"\n",
        "        Handles Buy/Sell logic including Cash deduction.\n",
        "        \"\"\"\n",
        "        if qty == 0: \n",
        "            return\n",
        "\n",
        "        # Calculate Cost & Commission\n",
        "        notional = abs(qty * price)\n",
        "        comm = notional * self.commission_bps \n",
        "        \n",
        "        self.cash -= (qty * price) \n",
        "        self.cash -= comm\n",
        "        \n",
        "        # Update Position \n",
        "        current_pos = self.positions.get(ticker, {'qty': 0, 'avg_price': 0.0})\n",
        "        curr_qty = current_pos['qty']\n",
        "        \n",
        "        # Check direction\n",
        "        is_same_direction = (curr_qty * qty) > 0 or curr_qty == 0\n",
        "        \n",
        "        if is_same_direction:\n",
        "            # scale in \n",
        "            new_total_qty = curr_qty + qty\n",
        "            old_notional = curr_qty * current_pos['avg_price']\n",
        "            new_trade_notional = qty * price\n",
        "        \n",
        "            new_avg_price = price\n",
        "            if new_total_qty != 0:\n",
        "                new_avg_price = (old_notional + new_trade_notional) / new_total_qty\n",
        "            \n",
        "            self.positions[ticker] = {'qty': new_total_qty, 'avg_price': new_avg_price}\n",
        "            \n",
        "        else:\n",
        "            # update quantity \n",
        "            new_total_qty = curr_qty + qty\n",
        "            if new_total_qty == 0:\n",
        "                if ticker in self.positions: del self.positions[ticker]\n",
        "            else:\n",
        "                is_flip = (curr_qty * new_total_qty) < 0\n",
        "                new_avg_price = price if is_flip else current_pos['avg_price']\n",
        "                self.positions[ticker] = {'qty': new_total_qty, 'avg_price': new_avg_price}\n",
        "\n",
        "        # log trade\n",
        "        self.history.append({\n",
        "            'date': date, 'ticker': ticker, 'action': action_label,\n",
        "            'qty': qty, 'price': price, 'comm': comm\n",
        "        })\n",
        "        \n",
        "def portfolio_analytics(equity: pd.Series, periods_per_year=252):\n",
        "    equity = equity.dropna()\n",
        "    rets = equity.pct_change().dropna()\n",
        "\n",
        "    # Sharpe (rf=0)\n",
        "    sharpe = np.nan\n",
        "    if rets.std() != 0:\n",
        "        sharpe = (rets.mean() / rets.std()) * np.sqrt(periods_per_year)\n",
        "\n",
        "    # Drawdown\n",
        "    peak = equity.cummax()\n",
        "    dd = equity / peak - 1.0\n",
        "    max_dd = dd.min()\n",
        "\n",
        "    # Max DD duration (in days)\n",
        "    underwater = equity < peak\n",
        "    dd_duration = (underwater.groupby((underwater != underwater.shift()).cumsum())\n",
        "                            .cumcount() + 1)\n",
        "    max_dd_duration = dd_duration[underwater].max() if underwater.any() else 0\n",
        "\n",
        "    # CAGR\n",
        "    years = (equity.index[-1] - equity.index[0]).days / 365.25\n",
        "    if years > 0:\n",
        "        cagr = (equity.iloc[-1] / equity.iloc[0]) ** (1/years) - 1 \n",
        "    else:\n",
        "        cagr = np.nan\n",
        "\n",
        "    # Vol (annualized)\n",
        "    ann_vol = rets.std() * np.sqrt(periods_per_year)\n",
        "\n",
        "    return {\n",
        "        \"Final Equity\": float(equity.iloc[-1]),\n",
        "        \"Total Return\": float(equity.iloc[-1] / equity.iloc[0] - 1),\n",
        "        \"CAGR\": float(cagr),\n",
        "        \"Ann Vol\": float(ann_vol),\n",
        "        \"Sharpe (rf=0)\": float(sharpe),\n",
        "        \"Max Drawdown\": float(max_dd),\n",
        "        \"Max DD Duration (days)\": int(max_dd_duration),\n",
        "    }, dd, rets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 5 - Conclusion\n",
        "1. Identify which pair produced the **highest cumulative P&L**.\n",
        "2. Summarize your results and provide an interpretation of which pair is the most likely to yield a profitable trading opportunity."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
